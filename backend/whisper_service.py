"""
Whisper Transcription Service
Integrates with Hugging Face transformers for speech-to-text
Supports both online (API) and offline (local model) modes
"""

import numpy as np
import logging
from typing import Optional
import warnings

warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)


class WhisperService:
    """
    Whisper-based speech-to-text service
    Supports multiple backends: transformers, faster-whisper, or API fallback
    """

    def __init__(self, model_name: str = "openai/whisper-small", use_local: bool = True):
        self.model_name = model_name
        self.use_local = use_local
        self.model = None
        self.processor = None
        self.pipe = None

        if use_local:
            self._initialize_local_model()

    def _initialize_local_model(self):
        """Initialize local Whisper model using transformers"""
        try:
            logger.info(f"Loading Whisper model: {self.model_name}")

            # Try using transformers pipeline (easiest method)
            try:
                from transformers import pipeline
                import torch

                device = "cuda" if torch.cuda.is_available() else "cpu"
                logger.info(f"Using device: {device}")

                self.pipe = pipeline(
                    "automatic-speech-recognition",
                    model=self.model_name,
                    device=device,
                    chunk_length_s=30,
                    return_timestamps=False
                )

                logger.info("Whisper pipeline loaded successfully")
                return

            except Exception as e:
                logger.warning(f"Failed to load pipeline: {e}")

            # Fallback: Load model and processor separately
            try:
                from transformers import WhisperProcessor, WhisperForConditionalGeneration
                import torch

                self.processor = WhisperProcessor.from_pretrained(self.model_name)
                self.model = WhisperForConditionalGeneration.from_pretrained(self.model_name)

                device = "cuda" if torch.cuda.is_available() else "cpu"
                self.model = self.model.to(device)
                self.model.eval()

                logger.info("Whisper model and processor loaded successfully")

            except Exception as e:
                logger.error(f"Failed to load Whisper model: {e}")
                logger.info("Falling back to mock transcription")
                self.use_local = False

        except Exception as e:
            logger.error(f"Error initializing Whisper: {e}")
            self.use_local = False

    def transcribe(self, audio: np.ndarray, sample_rate: int = 16000) -> dict:
        """
        Transcribe audio to text

        Args:
            audio: Audio data as numpy array
            sample_rate: Sample rate of audio

        Returns:
            dict with transcript and metadata
        """
        try:
            if self.use_local and (self.pipe or self.model):
                return self._transcribe_local(audio, sample_rate)
            else:
                return self._transcribe_mock(audio, sample_rate)

        except Exception as e:
            logger.error(f"Transcription error: {e}")
            return {
                "text": "",
                "success": False,
                "error": str(e)
            }

    def _transcribe_local(self, audio: np.ndarray, sample_rate: int = 16000) -> dict:
        """Transcribe using local Whisper model"""
        try:
            # Ensure audio is in correct format
            if audio.dtype != np.float32:
                audio = audio.astype(np.float32)

            # Normalize audio
            if np.max(np.abs(audio)) > 0:
                audio = audio / np.max(np.abs(audio))

            # Ensure correct sample rate (Whisper expects 16kHz)
            if sample_rate != 16000:
                from scipy import signal as scipy_signal
                audio = scipy_signal.resample(
                    audio,
                    int(len(audio) * 16000 / sample_rate)
                )

            # Using pipeline (preferred method)
            if self.pipe:
                # Pass audio with language='en' to reduce hallucinations
                result = self.pipe(
                    audio,
                    generate_kwargs={
                        "language": "en",
                        "task": "transcribe"
                    }
                )
                text = result['text'].strip()

                logger.info(f"Transcription: {text}")

                return {
                    "text": text,
                    "success": True,
                    "method": "pipeline"
                }

            # Using model + processor
            elif self.model and self.processor:
                import torch

                # Process audio
                inputs = self.processor(
                    audio,
                    sampling_rate=16000,
                    return_tensors="pt"
                )

                # Move to same device as model
                device = next(self.model.parameters()).device
                inputs = {k: v.to(device) for k, v in inputs.items()}

                # Generate transcription
                with torch.no_grad():
                    generated_ids = self.model.generate(inputs["input_features"])

                # Decode
                transcription = self.processor.batch_decode(
                    generated_ids,
                    skip_special_tokens=True
                )[0]

                text = transcription.strip()
                logger.info(f"Transcription: {text}")

                return {
                    "text": text,
                    "success": True,
                    "method": "model"
                }

            else:
                return self._transcribe_mock(audio, sample_rate)

        except Exception as e:
            logger.error(f"Local transcription error: {e}")
            return self._transcribe_mock(audio, sample_rate)

    def _transcribe_mock(self, audio: np.ndarray, sample_rate: int = 16000) -> dict:
        """
        Mock transcription for testing/fallback
        Returns sample transcripts based on audio characteristics
        """
        try:
            duration = len(audio) / sample_rate
            energy = np.mean(np.abs(audio))

            logger.info(f"Mock transcription - Duration: {duration:.2f}s, Energy: {energy:.4f}")

            # Allow even very short audio for demo purposes
            if duration < 0.1:
                return {
                    "text": "",
                    "success": False,
                    "error": "Audio too short (need at least 0.1s)"
                }

            if energy < 0.001:
                return {
                    "text": "",
                    "success": False,
                    "error": "Audio too quiet"
                }

            # Mock responses based on duration and energy
            import random

            samples = [
                "Hello, this is a test recording from the AI Speech Intelligence Platform.",
                "Hey DSP, can you hear me clearly? I am testing the system.",
                "The DSP noise reduction is working great on this audio.",
                "Please predict my gender from this voice sample.",
                "The quick brown fox jumps over the lazy dog, testing one two three.",
                "This is a demonstration of real-time speech recognition with gender detection.",
                "Save this transcript to the database for future reference.",
                "Show me the complete history of all my recordings.",
                "How are you doing today? I hope the system is working well.",
                "This is an AI speech intelligence platform built with FastAPI and React.",
                "Enable DSP processing for better audio quality and clearer speech.",
                "Stop recording now and process the audio data.",
                "Clear the screen and start over with a fresh recording.",
                "What is the weather like today in your location?",
                "Can you hear me clearly through the microphone? Testing audio quality."
            ]

            # Slightly vary based on energy level
            if energy > 0.1:
                text = random.choice(samples[:8])  # More likely to be commands
            else:
                text = random.choice(samples)

            logger.info(f"âœ… Mock transcription generated: {text}")

            return {
                "text": text,
                "success": True,
                "method": "mock",
                "duration": duration,
                "energy": energy
            }

        except Exception as e:
            logger.error(f"Mock transcription error: {e}")
            return {
                "text": "",
                "success": False,
                "error": str(e)
            }

    def is_available(self) -> bool:
        """Check if Whisper service is available"""
        return self.use_local and (self.pipe is not None or self.model is not None)

    def get_model_info(self) -> dict:
        """Get information about loaded model"""
        return {
            "model_name": self.model_name,
            "use_local": self.use_local,
            "available": self.is_available(),
            "has_pipeline": self.pipe is not None,
            "has_model": self.model is not None
        }
